# Data-Analysis-with-Python---Full-Course-for-Beginners-Numpy-Pandas-Matplotlib-Seaborn-
This is my project work from Free Code Camps' Data Analysis Course



## What is Data Analysis
Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making.


## Stack Learned:
- Python


## Why Python for Data Analysis?
### Why would we choose Python over R or Julia?
- Very simple and intuitive to learn  
- “correct” language  
- Powerful libraries (not just for Data Analysis)  
- Free and open source  
- Amazing community, docs and conferences  


### When to choose R?
- When R Studio is needed  
- When dealing with advanced statistical methods  
- When extreme performance is needed  


## The Data Analysis Process
### 1: Data Extraction
- SQL
- Scrapping
- File Formats
- CSV
- JSON
- XML
- Consulting APIs
- Buying Data
- Distributed Databases

### 2: Data Cleaning
- Missing values and empty data
- Data imputation
- Incorrect types
- Incorrect or invalid values
- Outliers and non relevant data
- Statistical sanitization

### 3: Data Wrangling
- Hierarchical Data
- Handling categorical data
- Reshaping and transforming structures
- Indexing data for quick access
- Merging, combining and joining data

### 4: Analysis
- Exploration
- Building statistical models
- Visualization and representations
- Correlation vs Causation analysis
- Hypothesis testing
- Statistical analysis
- Reporting

### 5: Action
- Building Machine Learning Models
- Feature Engineering
- Moving ML into production
- Building ETL pipelines
- Live dashboard and reporting
- Decision making and real-life tests

The process starts by getting the data. Where is your data coming from? Usually, it’s in your own database. But it could also come from files stored in different formats or web APIs.  

Once we’ve collected the data we’ll need to clean it. If the source of the data is your own database, then it’s probably already in shape. If you’re using more extreme sources, like web scraping, then the process will be more tedious.  

With our data cleaned, we’ll now need to rearrange and reshape the data for better analysis. Transforming fields, merging tables, combining data from multiple sources, etc. The objective of this process is to get the data ready for the next step.  

The process of analysis involves extracting patterns from the data that is now clean and in shape. Capturing trends or anomalies. Statistical analysis will be fundamental in this process.  

Finally, it’s time to do something with that analysis. If this was a Data Science project, we could be ready to implement Machine Learning models. If we focus strictly on Data Analysis, we’ll probably need to build reports, communicate our results and support decision making.  


Let me finish by saying that, in real life, this process isn’t so linear. We’re usually jumping back and forth between the steps, and it looks more like a cycle than a straight line.  
